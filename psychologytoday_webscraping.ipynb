{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pallavmarch/Advanced-SQL-50/blob/main/psychologytoday_webscraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Start                            \n"
      ],
      "metadata": {
        "id": "mPjT1mCKAVXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import link\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import files\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "# --------- Set this variable to scrape a specific state ---------\n",
        "selected_state = \"Texas\"  # Options: Texas, Florida, California, New York\n",
        "\n",
        "# --------- Configuration Dictionary for States ---------\n",
        "state_config = {\n",
        "    \"Texas\": {\n",
        "        \"base_url\": \"https://www.psychologytoday.com/us/therapists/texas?page=\",\n",
        "        \"link_file\": \"/content/therapist_link_texas.csv\",\n",
        "        \"data_file_1\": \"therapist_data_texas_1.csv\",\n",
        "        \"data_file_2\": \"therapist_data_texas_2.csv\"\n",
        "    },\n",
        "    \"Florida\": {\n",
        "        \"base_url\": \"https://www.psychologytoday.com/us/therapists/florida?page=\",\n",
        "        \"link_file\": \"/content/therapist_link_florida.csv\",\n",
        "        \"data_file_1\": \"therapist_data_florida_1.csv\",\n",
        "        \"data_file_2\": \"therapist_data_florida_2.csv\"\n",
        "    },\n",
        "    \"California\": {\n",
        "        \"base_url\": \"https://www.psychologytoday.com/us/therapists/california?page=\",\n",
        "        \"link_file\": \"/content/therapist_link_california.csv\",\n",
        "        \"data_file_1\": \"therapist_data_california_1.csv\",\n",
        "        \"data_file_2\": \"therapist_data_california_2.csv\"\n",
        "    },\n",
        "    \"New York\": {\n",
        "        \"base_url\": \"https://www.psychologytoday.com/us/therapists/new-york?page=\",\n",
        "        \"link_file\": \"/content/therapist_link_new_york.csv\",\n",
        "        \"data_file_1\": \"therapist_data_new_york_1.csv\",\n",
        "        \"data_file_2\": \"therapist_data_new_york_2.csv\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# --------- Use selected state config ---------\n",
        "config = state_config[selected_state]\n",
        "base_url = config[\"base_url\"]\n",
        "link_file = config[\"link_file\"]\n",
        "#file_name = config[\"data_file\"]\n",
        "\n",
        "# --------- Headers ---------\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# --------- Initialize Containers ---------\n",
        "all_therapists_data = []\n",
        "total_pages = 500  # Set this as per need\n",
        "\n",
        "all_data = []"
      ],
      "metadata": {
        "id": "WwWgCVMIuphe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPnk8nGZYE3T"
      },
      "source": [
        "# Part 1:\n",
        "Scraping the links of profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuqNOQoEIi5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e50b09-37f4-4134-d37b-c3dad3cc9534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Page Extraction Progress: 100%|\u001b[31m█████████████████████████████████\u001b[0m| 500/500 [51:24<00:00,  6.17s/page]\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset size: 10000\n",
            "Number of duplicates removed: 1776\n",
            "New dataset size: 8224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for page in tqdm(range(1, total_pages + 1), desc=\"Page Extraction Progress\", total=total_pages, unit=\"page\", colour=\"red\", ncols=100):\n",
        "    url = base_url + str(page)\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    time.sleep(random.uniform(1, 2))\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"❌ Failed to fetch page {page}. Status code:\", response.status_code)\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "\n",
        "    therapists = soup.find_all(\"div\", class_=\"results-row\")\n",
        "\n",
        "    for therapist in therapists:\n",
        "        try:\n",
        "\n",
        "            profile_link_element = therapist.find(\"a\", class_=\"profile-title\")\n",
        "            profile_url = profile_link_element.get(\"href\") if profile_link_element else \"N/A\"\n",
        "\n",
        "\n",
        "            all_data.append({\n",
        "                \"Profile URL\": profile_url\n",
        "                  })\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting data: {e}\")\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "\n",
        "print(f\"Original dataset size: {len(df)}\")\n",
        "duplicate_profiles = df[df['Profile URL'].duplicated(keep=False)]\n",
        "\n",
        "df_no_duplicates = df.drop_duplicates(subset='Profile URL', keep='first')\n",
        "num_duplicates_removed = len(df) - len(df.drop_duplicates(subset='Profile URL', keep='first'))\n",
        "\n",
        "df=df_no_duplicates\n",
        "print(f\"Number of duplicates removed: {num_duplicates_removed}\")\n",
        "print(f\"New dataset size: {len(df_no_duplicates)}\")\n",
        "\n",
        "df_no_duplicates.to_csv(link_file, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4ydtTZIqZ_3"
      },
      "source": [
        "# Part 2\n",
        "Scraping deatils of each profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpakXeW-Yh_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c18b2570-49ae-4a87-ce09-670f065f0fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of profile_urls_list_1: 7233\n",
            "Length of profile_urls_list_2: 7234\n",
            "Data file name: therapist_data_texas_2.csv\n"
          ]
        }
      ],
      "source": [
        "csvfile = pd.read_csv(link_file)\n",
        "\n",
        "\n",
        "split_index = len(csvfile) // 2\n",
        "profile_urls_list_1 = csvfile[\"Profile URL\"][:split_index].tolist()  # First half\n",
        "profile_urls_list_2 = csvfile[\"Profile URL\"][split_index:].tolist()  # Second half\n",
        "\n",
        "\n",
        "current_profiles=profile_urls_list_2  # Options: profile_urls_list_1  |  profile_urls_list_2\n",
        "\n",
        "\n",
        "\n",
        "if current_profiles == profile_urls_list_1:\n",
        "    file_name = config[\"data_file_1\"]\n",
        "elif current_profiles == profile_urls_list_2:\n",
        "    file_name = config[\"data_file_2\"]\n",
        "\n",
        "print(f\"Length of profile_urls_list_1: {len(profile_urls_list_1)}\")\n",
        "print(f\"Length of profile_urls_list_2: {len(profile_urls_list_2)}\")\n",
        "print(f\"Data file name: {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45EvXjg8OVny"
      },
      "outputs": [],
      "source": [
        "def extract_basic_info(soup, url):\n",
        "    data = {\n",
        "        \"Profile URL\": url,\n",
        "        \"Name\": \"Not Found\",\n",
        "        \"Title\": \"Not Found\",\n",
        "        \"Credential\": \"Not Found\",\n",
        "        \"Phone\": \"Not Found\",\n",
        "        \"Availability\": \"Not Found\"\n",
        "    }\n",
        "\n",
        "\n",
        "    name_tag = soup.find(\"h1\")\n",
        "    if name_tag:\n",
        "        data[\"Name\"] = name_tag.get_text(strip=True).lower()\n",
        "\n",
        "\n",
        "    profile_type = soup.find(\"span\", {\"data-x\": \"profile-suffix-profile-type\"})\n",
        "    if profile_type:\n",
        "        data[\"Title\"] = profile_type.get_text(strip=True)\n",
        "\n",
        "\n",
        "    academic_tags = soup.find_all(\"span\", {\"data-x\": \"profile-suffix-academic\"})\n",
        "    if academic_tags:\n",
        "        unique_credentials = list(set(tag.get_text(strip=True) for tag in academic_tags))\n",
        "        data[\"Credential\"] = \", \".join(sorted(unique_credentials))\n",
        "\n",
        "\n",
        "    phone_tag = soup.find(\"div\", class_=\"profile-phone\")\n",
        "    if phone_tag:\n",
        "        data[\"Phone\"] = phone_tag.get_text(strip=True)\n",
        "\n",
        "    availability_tag = soup.find(\"div\", class_=\"at-a-glance_row_appointments\")\n",
        "    if availability_tag:\n",
        "        availability_text = availability_tag.get_text(strip=True).lower()\n",
        "        if \"available\" in availability_text:\n",
        "            data[\"Availability\"] = availability_text.replace(\"available \", \"\")\n",
        "        else:\n",
        "            data[\"Availability\"] = availability_text\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def extract_list(soup, keyword):\n",
        "    section = soup.find(lambda tag: tag.name in [\"h2\", \"h3\", \"h4\"] and keyword.lower() in tag.get_text(strip=True).lower())\n",
        "    if section:\n",
        "        ul_section = section.find_next_sibling(\"ul\")\n",
        "        return \" | \".join(span.get_text(strip=True).lower() for span in ul_section.find_all(\"span\")) if ul_section else \"Not Found\"\n",
        "    return \"Not Found\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_fees_payment(soup):\n",
        "\n",
        "    data = {key: 0 for key in [\"Individual Sessions ($)\", \"Couple Sessions ($)\"]}\n",
        "    for item in soup.find_all(\"li\"):\n",
        "        text = item.get_text(strip=True)\n",
        "        if \"Individual Sessions\" in text:\n",
        "            data[\"Individual Sessions ($)\"] = re.findall(r\"\\d+\", text)[0] if re.findall(r\"\\d+\", text) else \"Not Found\"\n",
        "        elif \"Couple Sessions\" in text:\n",
        "            data[\"Couple Sessions ($)\"] = re.findall(r\"\\d+\", text)[0] if re.findall(r\"\\d+\", text) else \"Not Found\"\n",
        "#        elif \"Pay by\" in text:           data[\"Pay By\"] = text.replace(\"Pay by \", \"\").strip()\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def extract_specialties_expertise(soup):\n",
        "\n",
        "    data = {key: \"Not Found\" for key in [\"Top Specialties\", \"Expertise\"]}\n",
        "    specialty_section = soup.find(\"div\", id=\"specialty-attributes-section\")\n",
        "\n",
        "    if specialty_section:\n",
        "        for group in specialty_section.find_all(\"div\", class_=\"attributes-group\"):\n",
        "            heading = group.find(\"h3\")\n",
        "\n",
        "            if heading:\n",
        "                key = \"Top Specialties\" if \"Top Specialties\" in heading.get_text(strip=True) else \"Expertise\"\n",
        "\n",
        "                data[key] = \" | \".join(span.get_text(strip=True).lower() for span in group.find_all(\"span\", class_=\"attribute_base\"))\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def extract_types_of_therapy(soup):\n",
        "    therapy_section = soup.find(\"div\", id=\"treatment-approach-attributes-section\")\n",
        "    return \" | \".join(span.get_text(strip=True).lower() for span in therapy_section.find_all(\"span\", class_=\"attribute_base\")) if therapy_section else \"Not Found\"\n",
        "\n",
        "\n",
        "\n",
        "def extract_qualifications(soup):\n",
        "\n",
        "    data = {key: \"Not Found\" for key in [\"Membership\", \"Certificate\", \"Attended University\", \"Major/Degree\",\"Graduation Year\"]}\n",
        "    data[\"In Practice (years)\"] = 0\n",
        "#    qualifications_list = []\n",
        "\n",
        "    for item in soup.find_all(\"li\", class_=\"qualifications-element\"):\n",
        "        text = item.get_text(strip=True)\n",
        "\n",
        "#        qualifications_list.append(text)\n",
        "#        if \"Verified by\" in text:          data[\"Verified by Psychology Today\"] = \"Yes\"\n",
        "\n",
        "        if \"Membership\" in text:\n",
        "            data[\"Membership\"] = text.replace(\"Membership with\", \"\").strip().lower()\n",
        "        elif \"Certificate\" in text:\n",
        "            data[\"Certificate\"] = text.replace(\"Certificate from\", \"\").strip().lower()\n",
        "\n",
        "    for detail in soup.select(\"div.details span.primary-details\"):\n",
        "\n",
        "        text = detail.get_text(strip=True)\n",
        "        if \"In Practice\" in text:\n",
        "            data[\"In Practice (years)\"] = re.search(r\"\\d+\", text).group() if re.search(r\"\\d+\", text) else 0\n",
        "        elif \"Attended\" in text:\n",
        "            uni_match = re.search(r\"Attended (.*?)(?:,|$)\", text)\n",
        "            degree_match = re.search(r\",\\s(.*?)(?:,|\\sGraduated)\", text)\n",
        "            grad_match = re.search(r\"Graduated\\s*(\\d{4})\", text)\n",
        "\n",
        "            if uni_match:\n",
        "                data[\"Attended University\"] = uni_match.group(1).strip().lower()\n",
        "            if degree_match:\n",
        "                data[\"Major/Degree\"] = degree_match.group(1).strip().lower()\n",
        "            if grad_match:\n",
        "                data[\"Graduation Year\"] = grad_match.group(1).lower()\n",
        "\n",
        "#    data[\"Qualifications\"] = \" | \".join(qualifications_list) if qualifications_list else \"Not Found\"\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def extract_state_zip(soup):\n",
        "    data = {key: \"Not Found\" for key in [\"City\", \"State\", \"ZIP Code\"]}\n",
        "\n",
        "    address_section = soup.find(\"span\", class_=\"address-region address-text\")\n",
        "    if address_section:\n",
        "        address_text = address_section.get_text(strip=True)\n",
        "        match = re.match(r\"^(.*?),\\s*([A-Z]{2})\\s*(\\d{5})$\", address_text)\n",
        "        if match:\n",
        "            data[\"City\"] = match.group(1)\n",
        "            data[\"State\"] = match.group(2)\n",
        "            data[\"ZIP Code\"] = match.group(3)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_client_focus(soup):\n",
        "\n",
        "    data = {key: \"Not Found\" for key in [\"Age\", \"Participants\", \"Communities\", \"Religion\", \"I also speak\"]}\n",
        "    for section in soup.find_all(\"div\", class_=\"client-focus-tile\"):\n",
        "        heading_text = section.find(\"h3\").get_text(strip=True) if section.find(\"h3\") else \"\"\n",
        "        items = \" | \".join(span.get_text(strip=True).lower().replace(\",\", \"\") for span in section.find_all(\"span\", class_=\"client-focus-description\"))\n",
        "        for key in data.keys():\n",
        "            if key in heading_text:\n",
        "                data[key] = items\n",
        "    return data\n",
        "\n",
        "\n",
        "# \tdef extract_endorsements(soup):\n",
        "# \t    data = {\"Endorsement Count\": 0, \"Endorsed By\": \"Not Found\"}\n",
        "# \t    endorsement_badge = soup.find(\"div\", class_=\"endorsement-count clickable profile-badge\")\n",
        "# \t    if endorsement_badge:\n",
        "# \t        data[\"Endorsement Count\"] = int(re.search(r\"\\d+\", endorsement_badge.get_text(strip=True)).group()) if re.search(r\"\\d+\", endorsement_badge.get_text(strip=True)) else 0\n",
        "# \t    endorsers = [\n",
        "# \t        f\"{e.find('div', class_='title').get_text(strip=True)} - {e.find('div', class_='subtitle').get_text(strip=True)}\"\n",
        "# \t        for e in soup.find_all(\"div\", class_=\"endorsement\")\n",
        "# \t    ]\n",
        "# \t    data[\"Endorsed By\"] = \" | \".join(endorsers) if endorsers else \"Not Found\"\n",
        "# \t    return data\n",
        "\n",
        "#def extract_personal_statement(soup):\n",
        "#    statement_section = soup.find(\"div\", class_=\"personal-statement-container\")\n",
        "#    return \" \".join(p.get_text(strip=True) for p in statement_section.find_all(\"span\", class_=\"paragraph\")) if statement_section else \"Not Found\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmghbNUDYqu4",
        "outputId": "4874bbb9-86cd-4050-89be-a5a79ffbebe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping Progress:  45%|\u001b[31m█████████████▉                 \u001b[0m| 3261/7234 [1:53:03<2:07:29,  1.93s/profile]\u001b[0m"
          ]
        }
      ],
      "source": [
        "for idx, url in enumerate(tqdm(current_profiles, desc=\"Scraping Progress\", colour=\"red\", ncols=100,unit=\"profile\")):\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        time.sleep(random.uniform(1, 2))\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "#            h2 = soup.find(\"h2\")\n",
        "#            if h2:\n",
        "#                h2_text = h2.get_text(strip=True).lower().split('(')[0]\n",
        "#                h2_parts = [part.strip() for part in h2_text.split(',')]\n",
        "#                title = h2_parts[0] if len(h2_parts) > 0 else \"N/A\"\n",
        "#                credential = h2_parts[1] if len(h2_parts) > 1 else \"N/A\"\n",
        "#            else:\n",
        "#                title = \"N/A\"\n",
        "#                credential = \"N/A\"\n",
        "\n",
        "            therapist_data = {\n",
        "\n",
        "                **extract_basic_info(soup, url),\n",
        "                **extract_state_zip(soup),\n",
        "                **extract_fees_payment(soup),\n",
        "                \"Insurance\": extract_list(soup, \"Insurance\"),\n",
        "                **extract_specialties_expertise(soup),\n",
        "                \"Types of Therapy\": extract_types_of_therapy(soup),\n",
        "                **extract_qualifications(soup),\n",
        "                **extract_client_focus(soup)\n",
        "\n",
        "#                \"Profile URL\": url,\n",
        "#                \"Name\": name_tag.get_text(strip=True).lower() if name_tag else \"N/A\",\n",
        "#                #\"Title\": title,\n",
        "#                #\"Credential\": credential,\n",
        "#                \"Title\": title,\n",
        "#                \"Credential\": credential,\n",
        "#               \"Phone\": phone_tag.get_text(strip=True) if phone_tag else \"N/A\",\n",
        "#               \"Availability\": availability,\n",
        "#                **extract_endorsements(soup)\n",
        "#                \"Personal Statement\": extract_personal_statement(soup),\n",
        "\n",
        "            }\n",
        "\n",
        "            all_therapists_data.append(therapist_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error scraping {url}: {e}\")\n",
        "\n",
        "\n",
        "df = pd.DataFrame(all_therapists_data)\n",
        "df.to_csv(file_name, index=False)\n",
        "files.download(file_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge"
      ],
      "metadata": {
        "id": "l3KKk_nr2Vue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# --------- Set this variable to scrape a specific state ---------\n",
        "selected_state = \"new_york\"  # Options: texas, florida, california, new_york\n",
        "\n",
        "file_path = '/content/therapist_data_'+selected_state\n",
        "\n",
        "\n",
        "therapist_data_california = pd.concat([\n",
        "    pd.read_csv(file_path + \"_1.csv\"),\n",
        "    pd.read_csv(file_path + \"_2.csv\")\n",
        "], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "output_path = file_path + \".csv\"\n",
        "therapist_data_california.to_csv(output_path, index=False)\n",
        "\n",
        "\n",
        "files.download(output_path)"
      ],
      "metadata": {
        "id": "i0UX4Q882Vg4",
        "outputId": "1dc3b028-55af-4ab0-e5c0-8bde77d305dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_656bff66-9e76-411b-9681-0c89fd2e0b03\", \"therapist_data_new_york.csv\", 14915810)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "mPjT1mCKAVXn",
        "VPnk8nGZYE3T",
        "l4ydtTZIqZ_3"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}